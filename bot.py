import asyncio
import json
import logging
import math
import os
import re
import tempfile
from collections import deque
from dataclasses import dataclass
from datetime import datetime
from typing import List, Optional, Union

import aiohttp
import discord
from discord import app_commands
from discord.ext import commands
from dotenv import load_dotenv

try:
    from google import genai
    from google.genai import types
    GOOGLE_AVAILABLE = True
except ImportError:
    GOOGLE_AVAILABLE = False
    genai = None
    types = None

try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    OpenAI = None

load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

DEFAULT_MODEL_GOOGLE = "gemini-2.5-flash-lite"
DEFAULT_MODEL_OPENAI = "gpt-4o"
DEFAULT_BASE_URL = "https://api.openai.com/v1"


def env_bool(name: str, default: bool = False) -> bool:
    value = os.getenv(name)
    if value is None:
        return default
    return value.strip().lower() in {"1", "true", "yes", "y", "on"}


@dataclass
class Settings:
    channel_history_limit: int
    reply_chain_limit: int
    enable_web_search: bool
    search_provider: str
    enable_google_grounding: bool
    persona: Optional[str]
    enable_citation_links: bool
    embed_footer: str
    max_monthly_cost_usd: float
    max_requests_per_minute: int
    max_attachments_per_message: int
    max_attachment_size_mb: int
    usd_to_jpy_rate: float


def load_settings() -> Settings:
    channel_history_limit = int(os.getenv("CHANNEL_HISTORY_LIMIT", "50"))
    reply_chain_limit = int(os.getenv("REPLY_CHAIN_LIMIT", "50"))
    enable_web_search = env_bool("ENABLE_WEB_SEARCH", True)
    search_provider = os.getenv("SEARCH_PROVIDER", "tavily")
    enable_google_grounding = env_bool("ENABLE_GOOGLE_GROUNDING", True)
    persona = os.getenv("PERSONA")
    enable_citation_links = env_bool("ENABLE_CITATION_LINKS", False)
    embed_footer = os.getenv("EMBED_FOOTER", "[LMcord](https://github.com/lunae-f/LMcord), made with ‚ù§Ô∏è‚Äçüî• by Lunae. | MIT License")
    max_monthly_cost_usd = float(os.getenv("MAX_MONTHLY_COST_USD", "0"))
    max_requests_per_minute = int(os.getenv("MAX_REQUESTS_PER_MINUTE", "10"))
    max_attachments_per_message = int(os.getenv("MAX_ATTACHMENTS_PER_MESSAGE", "10"))
    max_attachment_size_mb = int(os.getenv("MAX_ATTACHMENT_SIZE_MB", "10"))
    usd_to_jpy_rate = float(os.getenv("USD_TO_JPY_RATE", "150"))
    return Settings(
        channel_history_limit=channel_history_limit,
        reply_chain_limit=reply_chain_limit,
        enable_web_search=enable_web_search,
        search_provider=search_provider,
        enable_google_grounding=enable_google_grounding,
        persona=persona,
        enable_citation_links=enable_citation_links,
        embed_footer=embed_footer,
        max_monthly_cost_usd=max_monthly_cost_usd,
        max_requests_per_minute=max_requests_per_minute,
        max_attachments_per_message=max_attachments_per_message,
        max_attachment_size_mb=max_attachment_size_mb,
        usd_to_jpy_rate=usd_to_jpy_rate,
    )


SETTINGS = load_settings()

@dataclass
class Profile:
    name: str
    platform: str
    model: str
    api_key: str
    base_url: Optional[str]
    input_cost_per_1m: float
    output_cost_per_1m: float


PROFILES_FILE = "profiles.json"
CHANNEL_PROFILES_FILE = "data/channel_profiles.json"
PROFILES: dict[str, Profile] = {}
ACTIVE_PROFILE_NAME: Optional[str] = None
CHANNEL_PROFILE_OVERRIDES: dict[int, str] = {}


def resolve_env_value(value: Optional[str]) -> str:
    if not value:
        return ""
    match = re.fullmatch(r"\$\{([A-Z0-9_]+)\}", value)
    if match:
        return os.getenv(match.group(1), "")
    return value


def load_profiles() -> None:
    global PROFILES, ACTIVE_PROFILE_NAME
    if not os.path.exists(PROFILES_FILE):
        raise RuntimeError(f"{PROFILES_FILE} is required")
    with open(PROFILES_FILE, "r", encoding="utf-8") as f:
        data = json.load(f)
    profiles = data.get("profiles", [])
    active_name = data.get("active_profile")
    if not profiles:
        raise RuntimeError("profiles.json must contain at least one profile")
    loaded: dict[str, Profile] = {}
    for p in profiles:
        name = p.get("name")
        platform = p.get("platform")
        model = p.get("model")
        api_key = resolve_env_value(p.get("api_key"))
        base_url = resolve_env_value(p.get("base_url")) or None
        input_cost = float(p.get("input_cost_per_1m", 0))
        output_cost = float(p.get("output_cost_per_1m", 0))
        if not name or not platform or not model:
            raise RuntimeError("Each profile must include name, platform, model")
        if platform not in {"google", "openai"}:
            raise RuntimeError(f"Unsupported platform in profile '{name}': {platform}")
        if not api_key:
            logger.warning(f"‚ö†Ô∏è „Éó„É≠„Éï„Ç°„Ç§„É´ '{name}' „ÅÆAPI„Ç≠„Éº„ÅåÊú™Ë®≠ÂÆö„ÅÆ„Åü„ÇÅ„Çπ„Ç≠„ÉÉ„Éó„Åó„Åæ„Åô")
            continue
        loaded[name] = Profile(
            name=name,
            platform=platform,
            model=model,
            api_key=api_key,
            base_url=base_url,
            input_cost_per_1m=input_cost,
            output_cost_per_1m=output_cost,
        )
    PROFILES = loaded
    if not PROFILES:
        raise RuntimeError("ÊúâÂäπ„Å™API„Ç≠„Éº„ÇíÊåÅ„Å§„Éó„É≠„Éï„Ç°„Ç§„É´„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì")
    ACTIVE_PROFILE_NAME = active_name or next(iter(loaded.keys()))
    if ACTIVE_PROFILE_NAME not in PROFILES:
        raise RuntimeError(f"active_profile '{ACTIVE_PROFILE_NAME}' not found in profiles.json")


def get_active_profile(channel_id: Optional[int] = None) -> Profile:
    if channel_id is not None:
        override = CHANNEL_PROFILE_OVERRIDES.get(channel_id)
        if override and override in PROFILES:
            return PROFILES[override]
    if not ACTIVE_PROFILE_NAME or ACTIVE_PROFILE_NAME not in PROFILES:
        raise RuntimeError("No active profile loaded")
    return PROFILES[ACTIVE_PROFILE_NAME]


def load_channel_profiles() -> None:
    """Load channel profile overrides from file."""
    global CHANNEL_PROFILE_OVERRIDES
    if not os.path.exists(CHANNEL_PROFILES_FILE):
        return
    try:
        with open(CHANNEL_PROFILES_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
            CHANNEL_PROFILE_OVERRIDES = {int(k): v for k, v in data.items()}
            logger.info(f"„ÉÅ„É£„É≥„Éç„É´„Éó„É≠„Éï„Ç°„Ç§„É´Ë®≠ÂÆö„ÇíË™≠„ÅøËæº„Åø„Åæ„Åó„Åü: {len(CHANNEL_PROFILE_OVERRIDES)}‰ª∂")
    except Exception as e:
        logger.error(f"„ÉÅ„É£„É≥„Éç„É´„Éó„É≠„Éï„Ç°„Ç§„É´„ÅÆË™≠„ÅøËæº„Åø„Å´Â§±Êïó„Åó„Åæ„Åó„Åü: {e}")


def save_channel_profiles() -> None:
    """Save channel profile overrides to file."""
    ensure_data_dir()
    try:
        with open(CHANNEL_PROFILES_FILE, "w", encoding="utf-8") as f:
            json.dump(CHANNEL_PROFILE_OVERRIDES, f)
    except Exception as e:
        logger.error(f"„ÉÅ„É£„É≥„Éç„É´„Éó„É≠„Éï„Ç°„Ç§„É´„ÅÆ‰øùÂ≠ò„Å´Â§±Êïó„Åó„Åæ„Åó„Åü: {e}")


def set_channel_profile(channel_id: int, profile_name: str) -> None:
    if profile_name not in PROFILES:
        raise RuntimeError(f"Unknown profile: {profile_name}")
    CHANNEL_PROFILE_OVERRIDES[channel_id] = profile_name
    save_channel_profiles()


def list_profiles() -> List[str]:
    return list(PROFILES.keys())


GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
DISCORD_TOKEN = os.getenv("DISCORD_TOKEN")
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")

if not DISCORD_TOKEN:
    raise RuntimeError("DISCORD_TOKEN is required")

load_profiles()
load_channel_profiles()

GOOGLE_CLIENTS: dict[str, genai.Client] = {}
OPENAI_CLIENTS: dict[tuple[str, str], OpenAI] = {}


def get_google_client(profile: Profile) -> genai.Client:
    if not GOOGLE_AVAILABLE:
        raise RuntimeError("google-genai package is not installed. Install with: pip install google-genai")
    if not profile.api_key:
        raise RuntimeError(f"API key missing for profile '{profile.name}'")
    if profile.api_key in GOOGLE_CLIENTS:
        return GOOGLE_CLIENTS[profile.api_key]
    client = genai.Client(api_key=profile.api_key)
    GOOGLE_CLIENTS[profile.api_key] = client
    return client


def get_openai_client(profile: Profile) -> OpenAI:
    if not OPENAI_AVAILABLE:
        raise RuntimeError("openai package is not installed. Install with: pip install openai")
    if not profile.api_key:
        raise RuntimeError(f"API key missing for profile '{profile.name}'")
    base_url = profile.base_url or DEFAULT_BASE_URL
    cache_key = (profile.api_key, base_url)
    if cache_key in OPENAI_CLIENTS:
        return OPENAI_CLIENTS[cache_key]
    client = OpenAI(api_key=profile.api_key, base_url=base_url)
    OPENAI_CLIENTS[cache_key] = client
    return client

intents = discord.Intents.default()
intents.message_content = True
intents.messages = True

bot = commands.Bot(command_prefix="!", intents=intents)
client = bot  # For backward compatibility

TOKENS_FILE = "data/tokens_monthly.json"

# Rate limiting (per user)
REQUEST_TIMES: dict[int, deque[datetime]] = {}
LAST_CLEANUP = datetime.now()

# Token update lock
TOKENS_LOCK = asyncio.Lock()

# Concurrent execution limits
MAX_CONCURRENT_LLM_CALLS = 5  # Maximum concurrent LLM API calls
MAX_CONCURRENT_FILE_UPLOADS = 3  # Maximum concurrent file uploads
LLM_SEMAPHORE: Optional[asyncio.Semaphore] = None
FILE_UPLOAD_SEMAPHORE: Optional[asyncio.Semaphore] = None

# Global aiohttp session
AIOHTTP_SESSION: Optional[aiohttp.ClientSession] = None


def ensure_data_dir() -> None:
    """Ensure data directory exists."""
    os.makedirs("data", exist_ok=True)


def load_monthly_tokens() -> dict:
    """Load monthly token counts from file."""
    ensure_data_dir()
    default_data = {"month": "", "input": 0, "output": 0, "cost": 0.0, "requests": 0}
    
    if not os.path.exists(TOKENS_FILE):
        return default_data
    
    try:
        with open(TOKENS_FILE, "r") as f:
            data = json.load(f)
            # Validate schema
            required_keys = ["month", "input", "output", "cost", "requests"]
            if not all(key in data for key in required_keys):
                logger.warning(f"„Éà„Éº„ÇØ„É≥„Éï„Ç°„Ç§„É´„ÅÆ„Çπ„Ç≠„Éº„Éû„Åå‰∏çÂÆåÂÖ®„Åß„Åô„ÄÇ„Éá„Éï„Ç©„É´„ÉàÂÄ§„Çí‰ΩøÁî®„Åó„Åæ„Åô„ÄÇ")
                return default_data
            return data
    except json.JSONDecodeError as e:
        logger.error(f"„Éà„Éº„ÇØ„É≥„Éï„Ç°„Ç§„É´„ÅÆJSONËß£Êûê„Å´Â§±Êïó„Åó„Åæ„Åó„Åü: {e}")
        return default_data
    except PermissionError as e:
        logger.error(f"„Éà„Éº„ÇØ„É≥„Éï„Ç°„Ç§„É´„ÅÆË™≠„ÅøËæº„ÅøÊ®©Èôê„Åå„ÅÇ„Çä„Åæ„Åõ„Çì: {e}")
        return default_data
    except Exception as e:
        logger.error(f"„Éà„Éº„ÇØ„É≥„Éï„Ç°„Ç§„É´„ÅÆË™≠„ÅøËæº„Åø„Å´Â§±Êïó„Åó„Åæ„Åó„Åü: {e}")
        return default_data


def save_monthly_tokens(data: dict) -> None:
    """Save monthly token counts to file."""
    ensure_data_dir()
    try:
        with open(TOKENS_FILE, "w") as f:
            json.dump(data, f)
    except Exception as e:
        logger.error(f"„Éà„Éº„ÇØ„É≥ÊÉÖÂ†±„ÅÆ‰øùÂ≠ò„Å´Â§±Êïó„Åó„Åæ„Åó„Åü: {e}")


def calculate_cost(input_tokens: int, output_tokens: int, profile: Profile) -> float:
    """Calculate cost based on tokens and profile pricing."""
    input_cost = (input_tokens / 1_000_000) * profile.input_cost_per_1m
    output_cost = (output_tokens / 1_000_000) * profile.output_cost_per_1m
    return round(input_cost + output_cost, 6)


def check_rate_limit(user_id: int) -> Optional[str]:
    """Check if user has exceeded rate limit. Returns error message if exceeded, None otherwise."""
    global LAST_CLEANUP
    
    now = datetime.now()
    cutoff = now.timestamp() - 60
    
    # 1Êó•1Âõû„ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„ÉóÔºà„É°„É¢„É™„É™„Éº„ÇØÂØæÁ≠ñÔºâ
    if (now - LAST_CLEANUP).total_seconds() > 86400:
        old_ids = [uid for uid, times in REQUEST_TIMES.items() if not times]
        for uid in old_ids:
            del REQUEST_TIMES[uid]
        LAST_CLEANUP = now
        logger.debug(f"Rate limit cleanup: removed {len(old_ids)} empty user entries")
    
    if user_id not in REQUEST_TIMES:
        REQUEST_TIMES[user_id] = deque()
    
    user_times = REQUEST_TIMES[user_id]
    while user_times and user_times[0].timestamp() < cutoff:
        user_times.popleft()
    
    if len(user_times) >= SETTINGS.max_requests_per_minute:
        return f"‚ö†Ô∏è „É¨„Éº„ÉàÂà∂Èôê: „ÅÇ„Å™„Åü„ÅØ1ÂàÜÈñì„Å´{SETTINGS.max_requests_per_minute}„É™„ÇØ„Ç®„Çπ„Éà„Åæ„Åß„Åß„Åô„ÄÇÂ∞ë„ÅóÂæÖ„Å£„Å¶„Åã„ÇâÂÜçË©¶Ë°å„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"
    
    user_times.append(now)
    return None


def get_cost_warning(tokens_data: dict) -> Optional[str]:
    """Check if cost warning threshold is reached. Returns warning message if applicable."""
    if SETTINGS.max_monthly_cost_usd <= 0:
        return None
    
    cost = tokens_data.get("cost", 0.0)
    cost_ratio = cost / SETTINGS.max_monthly_cost_usd
    
    if cost_ratio >= 0.95:
        return f"üö® **Ë≠¶Âëä**: ÊúàÈñì„Ç≥„Çπ„Éà‰∏äÈôê„ÅÆ95%„Å´ÈÅî„Åó„Åæ„Åó„Åü ({format_usd_cost(cost)} / {format_usd_cost(SETTINGS.max_monthly_cost_usd)})"
    elif cost_ratio >= 0.90:
        return f"‚ö†Ô∏è **Ë≠¶Âëä**: ÊúàÈñì„Ç≥„Çπ„Éà‰∏äÈôê„ÅÆ90%„Å´ÈÅî„Åó„Åæ„Åó„Åü ({format_usd_cost(cost)} / {format_usd_cost(SETTINGS.max_monthly_cost_usd)})"
    elif cost_ratio >= 0.80:
        return f"‚ö†Ô∏è Ê≥®ÊÑè: ÊúàÈñì„Ç≥„Çπ„Éà‰∏äÈôê„ÅÆ80%„Å´ÈÅî„Åó„Åæ„Åó„Åü ({format_usd_cost(cost)} / {format_usd_cost(SETTINGS.max_monthly_cost_usd)})"
    elif cost_ratio >= 0.50:
        return f"üí° ÊúàÈñì„Ç≥„Çπ„Éà‰∏äÈôê„ÅÆ50%„Å´ÈÅî„Åó„Åæ„Åó„Åü ({format_usd_cost(cost)} / {format_usd_cost(SETTINGS.max_monthly_cost_usd)})"
    
    return None


def update_monthly_tokens(input_tokens: int, output_tokens: int, profile: Profile) -> dict:
    """Update monthly token counts and reset if month changed. Must be called within TOKENS_LOCK."""
    current_month = datetime.now().strftime("%Y-%m")
    tokens_data = load_monthly_tokens()
    
    # Reset if month changed
    if tokens_data["month"] != current_month:
        tokens_data = {"month": current_month, "input": 0, "output": 0, "cost": 0.0, "requests": 0}
    
    tokens_data["input"] += input_tokens
    tokens_data["output"] += output_tokens
    tokens_data["cost"] = round(tokens_data.get("cost", 0.0) + calculate_cost(input_tokens, output_tokens, profile), 6)
    tokens_data["requests"] = tokens_data.get("requests", 0) + 1
    save_monthly_tokens(tokens_data)
    return tokens_data


async def update_bot_status(tokens_data: dict) -> None:
    """Update bot status with monthly token counts and cost."""
    if not client.user:
        return
    cost = tokens_data.get("cost", 0.0)
    cost_str = format_usd_cost(cost)
    jpy_str = format_jpy_cost(cost, SETTINGS.usd_to_jpy_rate)
    input_token_str = format_token_count(tokens_data['input'])
    output_token_str = format_token_count(tokens_data['output'])
    activity = discord.Activity(
        type=discord.ActivityType.playing,
        name=f"üí∏ {cost_str} {jpy_str} | üì• {input_token_str} ‚Ä¢ üì§ {output_token_str}"
    )
    await client.change_presence(activity=activity)


def build_system_prompt() -> str:
    # If persona is set, use only the persona
    if SETTINGS.persona:
        return SETTINGS.persona
    
    # Otherwise, use default assistant prompt
    base_prompt = "„ÅÇ„Å™„Åü„ÅØDiscord„ÅÆ„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô„ÄÇÊó•Êú¨Ë™û„Åß„ÄÅ‰∏ÅÂØß„Åã„Å§Ê≠£Á¢∫„ÅïÈáçË¶ñ„ÅßÂõûÁ≠î„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"
    if SETTINGS.enable_web_search or SETTINGS.enable_google_grounding:
        base_prompt += "\nÂøÖË¶Å„Å´Âøú„Åò„Å¶„Ç¶„Çß„ÉñÊ§úÁ¥¢„ÉÑ„Éº„É´„Çí‰ΩøÁî®„Åó„ÄÅÊúÄÊñ∞„ÅÆÊÉÖÂ†±„ÇíÂèñÂæó„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"
    base_prompt += "\nÊé®Ê∏¨„ÇíÈÅø„Åë„ÄÅ„Çè„Åã„Çâ„Å™„ÅÑÂ†¥Âêà„ÅØ„Çè„Åã„Çâ„Å™„ÅÑ„Å®‰ºù„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"
    
    return base_prompt


def format_message_content(message: discord.Message) -> str:
    parts = [message.content.strip()] if message.content else []
    if message.attachments:
        parts.extend(
            f"[Ê∑ª‰ªò] {a.filename} ({format_file_size(a.size)})" for a in message.attachments
        )
    content = "\n".join(p for p in parts if p)
    return content


def format_file_size(size_bytes: int) -> str:
    size_mb = size_bytes / (1024 * 1024)
    return f"{size_mb:.2f}MB"


def estimate_text_tokens(text: str) -> int:
    if not text:
        return 0
    return max(1, math.ceil(len(text) / 4))


def estimate_image_tokens(width: Optional[int], height: Optional[int]) -> int:
    if not width or not height:
        return 258
    if width <= 384 and height <= 384:
        return 258
    tiles_x = math.ceil(width / 768)
    tiles_y = math.ceil(height / 768)
    return 258 * tiles_x * tiles_y


def estimate_attachment_tokens(attachment: discord.Attachment) -> int:
    if attachment.content_type and attachment.content_type.startswith("image/"):
        return estimate_image_tokens(attachment.width, attachment.height)
    if attachment.width and attachment.height:
        return estimate_image_tokens(attachment.width, attachment.height)
    return max(1, math.ceil(attachment.size / 4))


def format_attachment_list(attachments: List[discord.Attachment]) -> str:
    return ", ".join(
        f"{a.filename} ({format_file_size(a.size)})" for a in attachments
    )


def validate_attachments(attachments: List[discord.Attachment], max_count: int, max_size_bytes: int) -> List[str]:
    errors: List[str] = []
    if len(attachments) > max_count:
        errors.append(
            f"Ê∑ª‰ªòÊï∞„Åå‰∏äÈôê({max_count}‰ª∂)„ÇíË∂Ö„Åà„Å¶„ÅÑ„Åæ„Åô: {format_attachment_list(attachments)}"
        )
    oversize = [a for a in attachments if a.size > max_size_bytes]
    if oversize:
        errors.append(
            f"„Çµ„Ç§„Ç∫‰∏äÈôê({format_file_size(max_size_bytes)})„ÇíË∂Ö„Åà„Å¶„ÅÑ„Åæ„Åô: {format_attachment_list(oversize)}"
        )
    return errors


async def build_gemini_file_parts(profile: Profile, attachments: List[discord.Attachment]) -> tuple[List[types.Part], int]:
    client = get_google_client(profile)
    parts: List[types.Part] = []
    estimated_tokens = 0
    max_retries = 3
    
    # Non-retryable error patterns
    non_retryable_errors = (
        "401", "unauthorized", "invalid api key",
        "403", "forbidden", "permission denied",
        "422", "unprocessable", "invalid file"
    )
    
    for attachment in attachments:
        estimated_tokens += estimate_attachment_tokens(attachment)
        data = await attachment.read()
        suffix = os.path.splitext(attachment.filename)[1]
        with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
            tmp.write(data)
            tmp_path = tmp.name
        
        uploaded = None
        last_error = None
        try:
            for attempt in range(max_retries):
                try:
                    async with FILE_UPLOAD_SEMAPHORE:
                        uploaded = await asyncio.to_thread(client.files.upload, file=tmp_path)
                    break
                except Exception as e:
                    last_error = e
                    error_str = str(e).lower()
                    
                    # Check if error is non-retryable
                    if any(pattern in error_str for pattern in non_retryable_errors):
                        logger.error(f"„Éï„Ç°„Ç§„É´„Ç¢„ÉÉ„Éó„É≠„Éº„ÉâÂç≥Â∫ßÂ§±ÊïóÔºà„É™„Éà„É©„Ç§‰∏çÂèØÔºâ: {attachment.filename} - {e}")
                        break
                    
                    logger.warning(f"„Éï„Ç°„Ç§„É´„Ç¢„ÉÉ„Éó„É≠„Éº„ÉâÂ§±Êïó (Ë©¶Ë°å {attempt + 1}/{max_retries}): {attachment.filename} - {e}")
                    if attempt < max_retries - 1:
                        await asyncio.sleep(2 ** attempt)
        finally:
            # Á¢∫ÂÆü„Å´‰∏ÄÊôÇ„Éï„Ç°„Ç§„É´„ÇíÂâäÈô§
            try:
                os.remove(tmp_path)
            except OSError as e:
                logger.warning(f"‰∏ÄÊôÇ„Éï„Ç°„Ç§„É´„ÅÆÂâäÈô§„Å´Â§±Êïó„Åó„Åæ„Åó„Åü: {tmp_path} - {e}")
        
        if not uploaded:
            raise RuntimeError(
                f"„Éï„Ç°„Ç§„É´„ÅÆ„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Å´Â§±Êïó„Åó„Åæ„Åó„ÅüÔºà{max_retries}ÂõûË©¶Ë°åÔºâ: {attachment.filename}\n"
                f"„Ç®„É©„Éº: {last_error}"
            )
        
        file_uri = getattr(uploaded, "uri", None) or getattr(uploaded, "file_uri", None)
        mime_type = getattr(uploaded, "mime_type", None) or attachment.content_type or "application/octet-stream"
        if not file_uri:
            raise RuntimeError(f"„Éï„Ç°„Ç§„É´„ÅÆ„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Å´Â§±Êïó„Åó„Åæ„Åó„Åü: {attachment.filename}")
        parts.append(types.Part(file_data=types.FileData(file_uri=file_uri, mime_type=mime_type)))
    
    return parts, estimated_tokens


def estimate_tokens_from_contents(contents: List[types.Content]) -> int:
    estimated = 0
    for content in contents:
        for part in content.parts:
            text = getattr(part, "text", None)
            if text:
                estimated += estimate_text_tokens(text)
    return estimated


def format_token_count(count: int) -> str:
    """Format token count with k suffix for values >= 1000."""
    if count >= 1000:
        return f"{count / 1000:.1f}k"
    return str(count)


def format_elapsed_time(seconds: float) -> str:
    """Format elapsed time in human-readable format."""
    if seconds < 1:
        return f"{seconds:.2f}s"
    elif seconds < 60:
        return f"{seconds:.1f}s"
    else:
        minutes = seconds / 60
        return f"{minutes:.1f}m"


def format_jpy_cost(usd_cost: float, rate: float) -> str:
    """Format cost in JPY with ‚âà prefix."""
    jpy_cost = usd_cost * rate
    if jpy_cost < 1:
        return f"(‚âà ¬•{jpy_cost:.2f})"
    return f"(‚âà ¬•{jpy_cost:.0f})"


def format_usd_cost(usd_cost: float) -> str:
    """Format cost in USD or cents."""
    if usd_cost < 0.10:
        cents = usd_cost * 100
        return f"{cents:.2f}¬¢"
    return f"${usd_cost:.4f}"


@dataclass
class BuildResult:
    payload: Union[List[types.Content], List[dict]]
    estimated_input_tokens: Optional[int] = None


def message_to_chat(message: discord.Message) -> Optional[dict]:
    content = format_message_content(message)
    if not content:
        return None
    role = "assistant" if message.author == client.user else "user"
    author = message.author.display_name
    return {
        "role": role,
        "content": f"{author}: {content}",
    }


async def fetch_reply_chain(message: discord.Message, limit: int) -> List[discord.Message]:
    chain: List[discord.Message] = []
    current = message
    while len(chain) < limit and current.reference is not None:
        try:
            if current.reference.resolved:
                referenced = current.reference.resolved
            else:
                referenced = await current.channel.fetch_message(current.reference.message_id)
        except Exception:
            break
        if referenced is None:
            break
        chain.append(referenced)
        current = referenced
    return list(reversed(chain))


def build_help_message(channel_id: Optional[int] = None) -> str:
    profile = get_active_profile(channel_id)
    settings_lines = [
        "ÁèæÂú®„ÅÆË®≠ÂÆö:",
        f"- „Éó„É≠„Éï„Ç°„Ç§„É´: {profile.name}",
        f"- „Éó„É©„ÉÉ„Éà„Éï„Ç©„Éº„É†: {profile.platform}",
        f"- „É¢„Éá„É´: {profile.model}",
    ]
    if profile.base_url:
        settings_lines.append(f"- „Ç®„É≥„Éâ„Éù„Ç§„É≥„Éà: {profile.base_url}")
    
    # Google GroundingË°®Á§∫Ôºàgoogle‰ΩøÁî®ÊôÇ„ÅÆ„ÅøÔºâ
    if profile.platform == "google":
        grounding_status = "ÊúâÂäπ" if SETTINGS.enable_google_grounding else "ÁÑ°Âäπ"
        settings_lines.append(f"- Google Grounding with Google Search: {grounding_status}")
    
    settings_lines.extend([
        f"- „ÉÅ„É£„É≥„Éç„É´Â±•Ê≠¥ÂèÇÁÖß: {SETTINGS.channel_history_limit}‰ª∂",
        f"- Ëøî‰ø°ÂèÇÁÖß„ÉÅ„Çß„Éº„É≥: {SETTINGS.reply_chain_limit}‰ª∂",
        f"- WebÊ§úÁ¥¢: {'ÊúâÂäπ' if SETTINGS.enable_web_search and TAVILY_API_KEY else 'ÁÑ°Âäπ'}",
        f"- Ê§úÁ¥¢„Éó„É≠„Éê„Ç§„ÉÄ: {SETTINGS.search_provider}",
        f"- Âá∫ÂÖ∏„É™„É≥„ÇØË°®Á§∫: {'ÊúâÂäπ' if SETTINGS.enable_citation_links else 'ÁÑ°Âäπ'}",
        f"- 1ÂàÜÈñì„ÅÆÊúÄÂ§ß„É™„ÇØ„Ç®„Çπ„ÉàÊï∞: {SETTINGS.max_requests_per_minute}",
        f"- ÊúàÈñì„Ç≥„Çπ„Éà‰∏äÈôê(USD): {SETTINGS.max_monthly_cost_usd}",
    ])
    settings_lines.append(
        f"- Ê∑ª‰ªò‰∏äÈôê: {SETTINGS.max_attachments_per_message}‰ª∂ / {SETTINGS.max_attachment_size_mb}MB/‰ª∂"
    )
    if profile.platform == "google":
        settings_lines.append("- Ê∑ª‰ªò„Éï„Ç°„Ç§„É´ÂÖ•Âäõ: ÊúâÂäπÔºàGemini Files APIÔºâ")
    else:
        settings_lines.append("- Ê∑ª‰ªò„Éï„Ç°„Ç§„É´ÂÖ•Âäõ: ÁÑ°ÂäπÔºàOpenAI„Åß„ÅØÊ∑ª‰ªò‰∏çÂèØÔºâ")
    
    # „Éö„É´„ÇΩ„Éä„ÇíÂÖ®ÊñáË°®Á§∫
    if SETTINGS.persona:
        settings_lines.extend([
            "",
            "„Äê„Éö„É´„ÇΩ„ÉäË®≠ÂÆö„Äë",
            SETTINGS.persona,
        ])
    
    settings_lines.extend([
        "",
        "‰Ωø„ÅÑÊñπ:",
        "- @„Éú„ÉÉ„ÉàÂêç Ë≥™ÂïèÂÜÖÂÆπ „ÅÆÂΩ¢Âºè„ÅßÂëº„Å≥Âá∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ",
        "- ÂøÖË¶Å„Å´Âøú„Åò„Å¶„Éú„ÉÉ„Éà„Åå„Ç¶„Çß„ÉñÊ§úÁ¥¢„Çí‰Ωø„ÅÑ„Åæ„ÅôÔºàÊ§úÁ¥¢„Åó„ÅüÂ†¥Âêà„ÅØÂèÇÁÖßURL„ÇíË°®Á§∫Ôºâ„ÄÇ",
    ])
    return "\n".join(settings_lines)


def split_discord_message(text: str, limit: int = 2000) -> List[str]:
    if len(text) <= limit:
        return [text]
    chunks: List[str] = []
    remaining = text
    while len(remaining) > limit:
        split_at = remaining.rfind("\n", 0, limit)
        if split_at == -1 or split_at < limit * 0.3:
            split_at = limit
        chunk = remaining[:split_at].rstrip()
        if chunk:
            chunks.append(chunk)
        remaining = remaining[split_at:].lstrip()
    if remaining:
        chunks.append(remaining)
    return chunks


async def tavily_search(query: str, max_results: int = 5) -> str:
    if not TAVILY_API_KEY:
        return "Tavily API„Ç≠„Éº„ÅåË®≠ÂÆö„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇ"
    
    if not AIOHTTP_SESSION:
        return "HTTP„Çª„ÉÉ„Ç∑„Éß„É≥„ÅåÂàùÊúüÂåñ„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇ"
    
    try:
        async with AIOHTTP_SESSION.post(
            "https://api.tavily.com/search",
            json={
                "api_key": TAVILY_API_KEY,
                "query": query,
                "max_results": max_results,
                "include_answer": True,
                "include_raw_content": False,
            },
            timeout=aiohttp.ClientTimeout(total=30),
        ) as response:
            response.raise_for_status()
            data = await response.json()
    except Exception as e:
        logger.error(f"TavilyÊ§úÁ¥¢„Ç®„É©„Éº: {e}")
        return f"Ê§úÁ¥¢„Å´Â§±Êïó„Åó„Åæ„Åó„Åü: {e}"
    
    answer = data.get("answer") or ""
    results = data.get("results") or []
    lines = []
    if answer:
        lines.append(f"Ë¶ÅÁ¥Ñ: {answer}")
    if results and SETTINGS.enable_citation_links:
        lines.append("ÂèÇÁÖß:")
        for item in results:
            title = item.get("title") or "(no title)"
            url = item.get("url") or ""
            lines.append(f"[{title}]({url})")
    return "\n".join(lines)


def build_tool_spec_google() -> List[types.Tool]:
    tools = []
    
    # Add Grounding with Google Search
    if SETTINGS.enable_google_grounding:
        tools.append(types.Tool(google_search=types.GoogleSearch()))
    
    # Add Tavily search as fallback
    if SETTINGS.enable_web_search and TAVILY_API_KEY and not SETTINGS.enable_google_grounding:
        tavily_search_declaration = types.FunctionDeclaration(
            name="tavily_search",
            description="ÂøÖË¶Å„Å´Âøú„Åò„Å¶„Ç¶„Çß„ÉñÊ§úÁ¥¢„ÇíË°å„ÅÑ„ÄÅË¶ÅÁ¥Ñ„Å®ÂèÇÁÖßURL„ÇíËøî„Åó„Åæ„Åô„ÄÇ",
            parameters={
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "Ê§úÁ¥¢„ÇØ„Ç®„É™",
                    },
                    "max_results": {
                        "type": "integer",
                        "description": "ÂèñÂæó„Åô„ÇãÁµêÊûúÊï∞",
                        "default": 5,
                    },
                },
                "required": ["query"],
            },
        )
        tools.append(types.Tool(function_declarations=[tavily_search_declaration]))
    
    return tools


def build_tool_spec_openai() -> Optional[List[dict]]:
    if not (SETTINGS.enable_web_search and TAVILY_API_KEY):
        return None
    
    return [
        {
            "type": "function",
            "function": {
                "name": "tavily_search",
                "description": "ÂøÖË¶Å„Å´Âøú„Åò„Å¶„Ç¶„Çß„ÉñÊ§úÁ¥¢„ÇíË°å„ÅÑ„ÄÅË¶ÅÁ¥Ñ„Å®ÂèÇÁÖßURL„ÇíËøî„Åó„Åæ„Åô„ÄÇ",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {
                            "type": "string",
                            "description": "Ê§úÁ¥¢„ÇØ„Ç®„É™",
                        },
                        "max_results": {
                            "type": "integer",
                            "description": "ÂèñÂæó„Åô„ÇãÁµêÊûúÊï∞",
                            "default": 5,
                        },
                    },
                    "required": ["query"],
                },
            },
        }
    ]


def extract_mention_prompt(content: str, bot_id: int) -> Optional[str]:
    pattern = rf"^<@!?{bot_id}>\s*"
    if not re.match(pattern, content.strip()):
        return None
    return re.sub(pattern, "", content.strip(), count=1).strip()


async def build_messages(message: discord.Message, prompt: str, profile: Profile) -> BuildResult:
    if profile.platform == "google":
        contents, estimated_input_tokens = await build_messages_google(message, prompt, profile)
        return BuildResult(payload=contents, estimated_input_tokens=estimated_input_tokens)
    messages = await build_messages_openai(message, prompt, profile)
    return BuildResult(payload=messages)


async def build_messages_google(message: discord.Message, prompt: str, profile: Profile) -> tuple[List[types.Content], int]:
    contents: List[types.Content] = []
    estimated_input_tokens = 0
    
    # System instruction part
    system_parts = [build_system_prompt()]
    
    # Reply chain
    reply_chain = await fetch_reply_chain(message, SETTINGS.reply_chain_limit)
    if reply_chain:
        lines = ["Ëøî‰ø°„ÉÅ„Çß„Éº„É≥ÂèÇÁÖß:"]
        for m in reply_chain:
            content = format_message_content(m)
            if content:
                lines.append(f"- {m.author.display_name}: {content}")
        system_parts.append("\n".join(lines))
    
    # Channel history
    history = [
        m
        async for m in message.channel.history(
            limit=SETTINGS.channel_history_limit, before=message
        )
    ]
    history.reverse()
    
    # Combine system prompt with context
    combined_system = "\n\n".join(system_parts)
    estimated_input_tokens += estimate_text_tokens(combined_system)
    contents.append(types.Content(role="user", parts=[types.Part(text=combined_system)]))
    contents.append(types.Content(role="model", parts=[types.Part(text="‰∫ÜËß£„Åó„Åæ„Åó„Åü„ÄÇ")]))
    
    # Add history
    for m in history:
        content_text = format_message_content(m)
        if content_text:
            role = "model" if m.author == client.user else "user"
            author_prefix = "" if m.author == client.user else f"{m.author.display_name}: "
            estimated_input_tokens += estimate_text_tokens(f"{author_prefix}{content_text}")
            contents.append(types.Content(role=role, parts=[types.Part(text=f"{author_prefix}{content_text}")]))
    
    # Add current prompt with attachments
    prompt_text = f"{message.author.display_name}: {prompt}"
    estimated_input_tokens += estimate_text_tokens(prompt_text)
    parts = [types.Part(text=prompt_text)]
    if message.attachments:
        file_parts, attachment_tokens = await build_gemini_file_parts(profile, message.attachments)
        parts.extend(file_parts)
        estimated_input_tokens += attachment_tokens
    contents.append(types.Content(role="user", parts=parts))
    
    return contents, estimated_input_tokens


async def build_messages_openai(message: discord.Message, prompt: str, profile: Profile) -> List[dict]:
    messages: List[dict] = [{"role": "system", "content": build_system_prompt()}]

    reply_chain = await fetch_reply_chain(message, SETTINGS.reply_chain_limit)
    if reply_chain:
        lines = ["Ëøî‰ø°„ÉÅ„Çß„Éº„É≥ÂèÇÁÖß:"]
        for m in reply_chain:
            content = format_message_content(m)
            if content:
                lines.append(f"- {m.author.display_name}: {content}")
        messages.append({"role": "system", "content": "\n".join(lines)})

    history = [
        m
        async for m in message.channel.history(
            limit=SETTINGS.channel_history_limit, before=message
        )
    ]
    history.reverse()
    for m in history:
        chat_message = message_to_chat(m)
        if chat_message:
            messages.append(chat_message)

    messages.append({"role": "user", "content": f"{message.author.display_name}: {prompt}"})
    return messages


async def run_llm(messages: BuildResult, profile: Profile) -> tuple[str, int, int, bool]:
    if profile.platform == "google":
        return await run_llm_google(messages.payload, profile, messages.estimated_input_tokens)
    return await run_llm_openai(messages.payload, profile)


async def run_llm_google(contents: List[types.Content], profile: Profile, estimated_input_tokens: Optional[int]) -> tuple[str, int, int, bool]:
    tools = build_tool_spec_google()
    client = get_google_client(profile)
    used_estimate = False
    
    async with LLM_SEMAPHORE:
        response = await asyncio.to_thread(
            client.models.generate_content,
            model=profile.model,
            contents=contents,
            config=types.GenerateContentConfig(
                tools=tools if tools else None,
            ),
        )
    
    # Get usage data
    input_tokens = 0
    output_tokens = 0
    if hasattr(response, 'usage_metadata') and response.usage_metadata:
        input_tokens = response.usage_metadata.prompt_token_count or 0
        output_tokens = response.usage_metadata.candidates_token_count or 0
    else:
        used_estimate = True
        estimated = estimated_input_tokens if estimated_input_tokens is not None else estimate_tokens_from_contents(contents)
        input_tokens = estimated
        output_tokens = estimate_text_tokens(response.text or "")
    
    # Extract grounding citations if available
    grounding_citations = ""
    if SETTINGS.enable_citation_links and response.candidates and response.candidates[0].grounding_metadata:
        metadata = response.candidates[0].grounding_metadata
        if hasattr(metadata, "grounding_chunks") and metadata.grounding_chunks:
            citations = []
            for chunk in metadata.grounding_chunks:
                if hasattr(chunk, "web") and chunk.web:
                    citations.append(f"[{chunk.web.title}]({chunk.web.uri})")
            if citations:
                grounding_citations = "\n\nÂèÇÁÖß: " + " | ".join(citations)
    
    # Check if there are function calls (for Tavily search fallback)
    if response.candidates and response.candidates[0].content.parts:
        first_part = response.candidates[0].content.parts[0]
        if hasattr(first_part, 'function_call') and first_part.function_call:
            # Process function calls
            contents.append(response.candidates[0].content)
            
            estimated_follow_up_input = 0
            for part in response.candidates[0].content.parts:
                if hasattr(part, 'function_call') and part.function_call:
                    fc = part.function_call
                    if fc.name == "tavily_search":
                        query = fc.args.get("query", "")
                        max_results = int(fc.args.get("max_results", 5))
                        tool_result = await tavily_search(query, max_results=max_results)
                        estimated_follow_up_input += estimate_text_tokens(query)
                        estimated_follow_up_input += estimate_text_tokens(tool_result)
                        
                        # Add function response
                        contents.append(
                            types.Content(
                                role="user",
                                parts=[
                                    types.Part(
                                        function_response=types.FunctionResponse(
                                            name="tavily_search",
                                            response={"result": tool_result}
                                        )
                                    )
                                ]
                            )
                        )
            
            # Get follow-up response
            async with LLM_SEMAPHORE:
                follow_up = await asyncio.to_thread(
                    client.models.generate_content,
                    model=profile.model,
                    contents=contents,
                )
            # Update token counts for follow-up
            if hasattr(follow_up, 'usage_metadata') and follow_up.usage_metadata:
                input_tokens += follow_up.usage_metadata.prompt_token_count or 0
                output_tokens += follow_up.usage_metadata.candidates_token_count or 0
            else:
                used_estimate = True
                input_tokens += estimated_follow_up_input
                output_tokens += estimate_text_tokens(follow_up.text or "")
            return (follow_up.text or "") + grounding_citations, input_tokens, output_tokens, used_estimate
    
    return (response.text or "") + grounding_citations, input_tokens, output_tokens, used_estimate


async def run_llm_openai(messages: List[dict], profile: Profile) -> tuple[str, int, int, bool]:
    tools = build_tool_spec_openai()
    client = get_openai_client(profile)
    
    async with LLM_SEMAPHORE:
        response = await asyncio.to_thread(
            client.chat.completions.create,
            model=profile.model,
            messages=messages,
            tools=tools,
            tool_choice="auto" if tools else None,
        )
    
    # Get usage data
    input_tokens = response.usage.prompt_tokens if response.usage else 0
    output_tokens = response.usage.completion_tokens if response.usage else 0
    
    message = response.choices[0].message

    if message.tool_calls:
        messages.append({
            "role": "assistant",
            "content": message.content or "",
            "tool_calls": [
                {
                    "id": tool_call.id,
                    "type": tool_call.type,
                    "function": {
                        "name": tool_call.function.name,
                        "arguments": tool_call.function.arguments,
                    },
                }
                for tool_call in message.tool_calls
            ],
        })
        for tool_call in message.tool_calls:
            if tool_call.function.name == "tavily_search":
                args = json.loads(tool_call.function.arguments or "{}")
                query = args.get("query", "")
                max_results = int(args.get("max_results", 5))
                tool_result = await tavily_search(query, max_results=max_results)
                messages.append(
                    {
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "content": tool_result,
                    }
                )
        async with LLM_SEMAPHORE:
            follow_up = await asyncio.to_thread(
                client.chat.completions.create,
                model=profile.model,
                messages=messages,
            )
        # Accumulate token counts from follow-up
        if follow_up.usage:
            input_tokens += follow_up.usage.prompt_tokens
            output_tokens += follow_up.usage.completion_tokens
        return follow_up.choices[0].message.content or "", input_tokens, output_tokens, False

    return message.content or "", input_tokens, output_tokens, False


@client.event
async def on_ready() -> None:
    global LLM_SEMAPHORE, FILE_UPLOAD_SEMAPHORE, AIOHTTP_SESSION
    
    # Initialize semaphores
    LLM_SEMAPHORE = asyncio.Semaphore(MAX_CONCURRENT_LLM_CALLS)
    FILE_UPLOAD_SEMAPHORE = asyncio.Semaphore(MAX_CONCURRENT_FILE_UPLOADS)
    
    # Initialize global aiohttp session
    AIOHTTP_SESSION = aiohttp.ClientSession()
    
    print(f"Logged in as {client.user}")
    # Set initial status
    tokens_data = load_monthly_tokens()
    await update_bot_status(tokens_data)
    
    # Sync slash commands
    try:
        synced = await bot.tree.sync()
        logger.info(f"Synced {len(synced)} command(s)")
    except Exception as e:
        logger.error(f"Failed to sync commands: {e}")


lmcord_group = app_commands.Group(name="lmcord", description="LMcord„ÅÆÊìç‰Ωú„Ç≥„Éû„É≥„Éâ")


@lmcord_group.command(name="help", description="ÁèæÂú®„ÅÆË®≠ÂÆö„Å®„Éò„É´„Éó„ÇíË°®Á§∫")
async def llmcord_help(interaction: discord.Interaction):
    help_text = build_help_message(interaction.channel_id)
    await interaction.response.send_message(help_text, ephemeral=True)


@lmcord_group.command(name="profiles", description="Âà©Áî®ÂèØËÉΩ„Å™„Éó„É≠„Éï„Ç°„Ç§„É´‰∏ÄË¶ß„ÇíË°®Á§∫")
async def llmcord_profiles(interaction: discord.Interaction):
    channel_id = interaction.channel_id
    active_default = ACTIVE_PROFILE_NAME or "(none)"
    active_channel = CHANNEL_PROFILE_OVERRIDES.get(channel_id) if channel_id else None
    lines = ["Âà©Áî®ÂèØËÉΩ„Å™„Éó„É≠„Éï„Ç°„Ç§„É´:"]
    for name in list_profiles():
        marks = []
        if name == active_default:
            marks.append("default")
        if active_channel and name == active_channel:
            marks.append("channel")
        suffix = f" ({', '.join(marks)})" if marks else ""
        lines.append(f"- {name}{suffix}")
    await interaction.response.send_message("\n".join(lines), ephemeral=True)


@lmcord_group.command(name="switch", description="„Åì„ÅÆ„ÉÅ„É£„É≥„Éç„É´„ÅÆ„Éó„É≠„Éï„Ç°„Ç§„É´„ÇíÂàá„ÇäÊõø„Åà")
@app_commands.describe(profile="Âàá„ÇäÊõø„ÅàÂÖà„Éó„É≠„Éï„Ç°„Ç§„É´")
@app_commands.choices(profile=[
    app_commands.Choice(name=name, value=name) for name in list_profiles()
])
async def llmcord_switch(interaction: discord.Interaction, profile: str):
    if not interaction.channel_id:
        await interaction.response.send_message("„ÉÅ„É£„É≥„Éç„É´ÊÉÖÂ†±„ÅåÂèñÂæó„Åß„Åç„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ", ephemeral=True)
        return
    try:
        set_channel_profile(interaction.channel_id, profile)
        await interaction.response.send_message(f"„Åì„ÅÆ„ÉÅ„É£„É≥„Éç„É´„ÅÆ„Éó„É≠„Éï„Ç°„Ç§„É´„Çí '{profile}' „Å´Âàá„ÇäÊõø„Åà„Åæ„Åó„Åü„ÄÇ")
    except Exception as exc:
        await interaction.response.send_message(f"Âàá„ÇäÊõø„Åà„Å´Â§±Êïó„Åó„Åæ„Åó„Åü: {exc}", ephemeral=True)


@lmcord_group.command(name="stats", description="ÊúàÊ¨°Áµ±Ë®àÊÉÖÂ†±„ÇíË°®Á§∫")
async def llmcord_stats(interaction: discord.Interaction):
    tokens_data = load_monthly_tokens()
    month = tokens_data.get("month", "N/A")
    input_tokens = tokens_data.get("input", 0)
    output_tokens = tokens_data.get("output", 0)
    cost = tokens_data.get("cost", 0.0)
    requests = tokens_data.get("requests", 0)
    
    cost_str = format_usd_cost(cost)
    jpy_str = format_jpy_cost(cost, SETTINGS.usd_to_jpy_rate)
    input_token_str = format_token_count(input_tokens)
    output_token_str = format_token_count(output_tokens)
    
    avg_input = input_tokens // requests if requests > 0 else 0
    avg_output = output_tokens // requests if requests > 0 else 0
    avg_cost = cost / requests if requests > 0 else 0.0
    avg_input_str = format_token_count(avg_input)
    avg_output_str = format_token_count(avg_output)
    avg_cost_str = format_usd_cost(avg_cost)
    avg_jpy_str = format_jpy_cost(avg_cost, SETTINGS.usd_to_jpy_rate)
    
    stats_lines = [
        f"üìä ÊúàÊ¨°Áµ±Ë®àÊÉÖÂ†± ({month})",
        "",
        f"üí∏ Á∑è„Ç≥„Çπ„Éà: {cost_str} {jpy_str}",
        f"üì• Á∑èÂÖ•Âäõ„Éà„Éº„ÇØ„É≥: {input_token_str}",
        f"üì§ Á∑èÂá∫Âäõ„Éà„Éº„ÇØ„É≥: {output_token_str}",
        f"üìã Á∑è„É™„ÇØ„Ç®„Çπ„ÉàÊï∞: {requests}",
        "",
        f"üí∏ Âπ≥Âùá„Ç≥„Çπ„Éà: {avg_cost_str} {avg_jpy_str}",
        f"üì• Âπ≥ÂùáÂÖ•Âäõ„Éà„Éº„ÇØ„É≥: {avg_input_str}",
        f"üì§ Âπ≥ÂùáÂá∫Âäõ„Éà„Éº„ÇØ„É≥: {avg_output_str}",
    ]
    
    await interaction.response.send_message("\n".join(stats_lines), ephemeral=True)


bot.tree.add_command(lmcord_group)


@client.event
async def on_message(message: discord.Message) -> None:
    if message.author.bot:
        return
    if not client.user:
        return

    prompt = extract_mention_prompt(message.content, client.user.id)
    if prompt is None:
        return

    if not prompt:
        await message.channel.send(build_help_message(message.channel.id))
        return

    # Check rate limit (per user)
    rate_limit_error = check_rate_limit(message.author.id)
    if rate_limit_error:
        await message.channel.send(rate_limit_error, reference=message)
        return

    # Check monthly cost limit
    if SETTINGS.max_monthly_cost_usd > 0:
        tokens_data = load_monthly_tokens()
        cost_ratio = tokens_data["cost"] / SETTINGS.max_monthly_cost_usd
        if cost_ratio >= 1.0:
            await message.channel.send(
                f"üö® ÊúàÈñì„Ç≥„Çπ„Éà‰∏äÈôê({format_usd_cost(SETTINGS.max_monthly_cost_usd)})„Å´ÈÅî„Åó„Åæ„Åó„Åü„ÄÇ"
                "Êñ∞„Åó„ÅÑ„É™„ÇØ„Ç®„Çπ„Éà„ÅØÂèó„Åë‰ªò„Åë„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇ",
                reference=message
            )
            return

    profile = get_active_profile(message.channel.id)

    attachments = list(message.attachments or [])
    if attachments:
        max_size_bytes = SETTINGS.max_attachment_size_mb * 1024 * 1024
        errors = validate_attachments(
            attachments,
            SETTINGS.max_attachments_per_message,
            max_size_bytes,
        )
        if errors:
            error_lines = ["Ê∑ª‰ªò„Éï„Ç°„Ç§„É´„ÅÆÂà∂Èôê„Å´„Çà„ÇäÂá¶ÁêÜ„Åß„Åç„Åæ„Åõ„Çì„ÄÇ"]
            error_lines.extend(f"- {err}" for err in errors)
            await message.channel.send("\n".join(error_lines), reference=message)
            return
        if profile.platform == "openai":
            error_lines = [
                "OpenAI„Éó„É≠„Éï„Ç°„Ç§„É´„Åß„ÅØÊ∑ª‰ªò„Éï„Ç°„Ç§„É´ÂÖ•Âäõ„Å´ÂØæÂøú„Åó„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇ",
                "Ê∑ª‰ªò„ÇíÂâäÈô§„Åó„Å¶ÂÜçÈÄÅ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ",
                f"Ê∑ª‰ªò: {format_attachment_list(attachments)}",
            ]
            await message.channel.send("\n".join(error_lines), reference=message)
            return

    # Log incoming message
    logger.info(f"üì® Message from {message.author.name} in #{message.channel.name} [{profile.name}]: {prompt[:100]}...")

    await message.channel.typing()
    start_time = datetime.now()
    try:
        msg_data = await build_messages(message, prompt, profile)
        reply, input_tokens, output_tokens, used_estimate = await run_llm(msg_data, profile)
        elapsed_time = (datetime.now() - start_time).total_seconds()
        if not reply:
            reply = "Áî≥„ÅóË®≥„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÅÂøúÁ≠î„ÅÆÁîüÊàê„Å´Â§±Êïó„Åó„Åæ„Åó„Åü„ÄÇ"
        
        # Log response
        estimate_flag = "Êé®ÂÆö" if used_estimate else "ÂÆüÊ∏¨"
        logger.info(
            f"‚úÖ Responded to {message.author.name} [{profile.name}] ({input_tokens} in, {output_tokens} out, {estimate_flag})"
        )
        
        chunks = split_discord_message(reply)
        for i, chunk in enumerate(chunks):
            if i == 0:
                await message.channel.send(chunk, reference=message)
            else:
                await message.channel.send(chunk)
        
        # Send token count as an embed
        cost = calculate_cost(input_tokens, output_tokens, profile)
        cost_str = format_usd_cost(cost)
        jpy_str = format_jpy_cost(cost, SETTINGS.usd_to_jpy_rate)
        input_token_str = format_token_count(input_tokens)
        output_token_str = format_token_count(output_tokens)
        elapsed_str = format_elapsed_time(elapsed_time)
        estimate_suffix = " (Êé®ÂÆö)" if used_estimate else ""
        embed = discord.Embed(
            color=discord.Color.greyple(),
            description=f"üí∏ {cost_str} {jpy_str} | üì• {input_token_str} ‚Ä¢ üì§ {output_token_str} ‚Ä¢ ‚è±Ô∏è {elapsed_str}{estimate_suffix}\n{SETTINGS.embed_footer}"
        )
        await message.channel.send(embed=embed)
        
        # Update monthly token counts and bot status (with lock)
        async with TOKENS_LOCK:
            tokens_data = update_monthly_tokens(input_tokens, output_tokens, profile)
        await update_bot_status(tokens_data)
        
        # Check and send cost warning
        cost_warning = get_cost_warning(tokens_data)
        if cost_warning:
            await message.channel.send(cost_warning)
    except Exception as exc:
        # Log error
        logger.error(f"‚ùå Error processing message from {message.author.name}: {exc}", exc_info=True)
        await message.channel.send(
            f"„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {exc}", reference=message
        )


async def main() -> None:
    try:
        await bot.start(DISCORD_TOKEN)
    finally:
        # Cleanup global aiohttp session
        if AIOHTTP_SESSION and not AIOHTTP_SESSION.closed:
            await AIOHTTP_SESSION.close()


if __name__ == "__main__":
    asyncio.run(main())
